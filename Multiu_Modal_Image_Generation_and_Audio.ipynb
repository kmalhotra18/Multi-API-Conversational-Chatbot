{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxxBEw0MqdIMQG9oTgZ0+t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalhotra18/Multi-API-Conversational-Chatbot/blob/main/Multiu_Modal_Image_Generation_and_Audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sgpott79y1bj"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic"
      ],
      "metadata": {
        "id": "HCv_fuXby4Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import List\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import google.generativeai\n",
        "import anthropic\n",
        "import json"
      ],
      "metadata": {
        "id": "Mi0EwwO9y5G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "QjEqInu0y6ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr # Import Gradio for UI"
      ],
      "metadata": {
        "id": "o3_kkFB1y8Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load environment variables in a file called .env\n",
        "# Print the key prefixes to help with any debugging\n",
        "\n",
        "load_dotenv(override=True)\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "_AqUGJD4y9f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "\n",
        "if anthropic_api_key:\n",
        "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
        "else:\n",
        "    print(\"Anthropic API Key not set\")\n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"Google API Key not set\")"
      ],
      "metadata": {
        "id": "HLNZdnOWy-zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialization\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "openai = OpenAI()\n"
      ],
      "metadata": {
        "id": "4fLTy8pNzAX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are a helpful assistant for an Airline called FlightAI. \"\n",
        "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
        "system_message += \"Always be accurate. If you don't know the answer, say so.\""
      ],
      "metadata": {
        "id": "zOddKicF0KNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function looks rather simpler than the one from my video, because we're taking advantage of the latest Gradio updates\n",
        "\n",
        "# def chat(message, history):\n",
        "#     messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "#     response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
        "#     return response.choices[0].message.content\n",
        "\n",
        "# gr.ChatInterface(fn=chat, type=\"messages\").launch()"
      ],
      "metadata": {
        "id": "oRFv-V7R0LwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start by making a useful function\n",
        "\n",
        "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
        "\n",
        "def get_ticket_price(destination_city):\n",
        "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
        "    city = destination_city.lower()\n",
        "    return ticket_prices.get(city, \"Unknown\")"
      ],
      "metadata": {
        "id": "69VrzwDB0PzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ticket_price(\"London\")"
      ],
      "metadata": {
        "id": "a9Lbc0fc0QtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There's a particular dictionary structure that's required to describe our function:\n",
        "\n",
        "price_function = {\n",
        "    \"name\": \"get_ticket_price\",\n",
        "    \"description\": \"Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"destination_city\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The city that the customer wants to travel to\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"destination_city\"],\n",
        "        \"additionalProperties\": False\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "b4uTRfYc0SAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And this is included in a list of tools:\n",
        "\n",
        "tools = [{\"type\": \"function\", \"function\": price_function}]"
      ],
      "metadata": {
        "id": "K1rI78-r0TTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message, history):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
        "\n",
        "    if response.choices[0].finish_reason==\"tool_calls\":\n",
        "        message = response.choices[0].message\n",
        "        response, city = handle_tool_call(message)\n",
        "        messages.append(message)\n",
        "        messages.append(response)\n",
        "        response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "jSmRXpiH0UZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We have to write that function handle_tool_call:\n",
        "\n",
        "def handle_tool_call(message):\n",
        "    tool_call = message.tool_calls[0]\n",
        "    arguments = json.loads(tool_call.function.arguments)\n",
        "    city = arguments.get('destination_city')\n",
        "    price = get_ticket_price(city)\n",
        "    response = {\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": json.dumps({\"destination_city\": city,\"price\": price}),\n",
        "        \"tool_call_id\": tool_call.id\n",
        "    }\n",
        "    return response, city"
      ],
      "metadata": {
        "id": "8edrXmwa0Wes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
      ],
      "metadata": {
        "id": "BibByYRA0hqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets go Multi-Modal!**"
      ],
      "metadata": {
        "id": "fjxbWxq90XjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use DALL-E-3, the image generation model behind GPT-4o, to make us some images"
      ],
      "metadata": {
        "id": "3IYCcV6T0cIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some imports for handling images\n",
        "\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image           #Python Image Library"
      ],
      "metadata": {
        "id": "trpx284N0dZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def artist(city):\n",
        "    image_response = openai.images.generate(\n",
        "            model=\"dall-e-3\",                       #Dall-e-3 model\n",
        "            prompt=f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant pop-art style\", #prompt to display kind of image\n",
        "            size=\"1024x1024\",                       #size of image\n",
        "            n=1,                                    #number of images to generate\n",
        "            response_format=\"b64_json\",              #format of response\n",
        "        )\n",
        "    image_base64 = image_response.data[0].b64_json\n",
        "    image_data = base64.b64decode(image_base64)          #Decode image to bytes\n",
        "    return Image.open(BytesIO(image_data))"
      ],
      "metadata": {
        "id": "VomZz5qT0fL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = artist(\"New York City\")\n",
        "display(image)"
      ],
      "metadata": {
        "id": "3wRcYIc60asI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a function talker that uses **OpenAI's speech model** to generate Audio"
      ],
      "metadata": {
        "id": "lg7TGvoq08L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "from pydub.playback import play"
      ],
      "metadata": {
        "id": "NtCzv-0Z0yyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# def talker(message):\n",
        "#     response = openai.audio.speech.create(\n",
        "#         model=\"tts-1\",\n",
        "#         voice=\"alloy\",                          #onxy is another sound\n",
        "#         input=message)\n",
        "\n",
        "#     audio_stream = BytesIO(response.content)\n",
        "#     output_filename = \"output_audio.mp3\"\n",
        "#     with open(output_filename, \"wb\") as f:\n",
        "#         f.write(audio_stream.read())\n",
        "\n",
        "#     # Play the generated audio\n",
        "#     display(Audio(output_filename, autoplay=True))\n",
        "\n",
        "# talker(\"Well, hi there\")\n",
        "\n",
        "\n",
        "def talker(message):\n",
        "    response = openai.audio.speech.create(\n",
        "        model=\"tts-1\",\n",
        "        voice=\"alloy\",\n",
        "        input=message\n",
        "    )\n",
        "    audio_stream = BytesIO(response.content)\n",
        "    output_filename = \"output_audio.mp3\"\n",
        "    with open(output_filename, \"wb\") as f:\n",
        "        f.write(audio_stream.read())\n",
        "\n",
        "    return output_filename  # Return path for Gradio\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "j7OVZ28X2UWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agent Framework**"
      ],
      "metadata": {
        "id": "a5TefO2DUeIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def chat(history):                                                                              #usual chat function that takes message and history for OpenAi and calls response\n",
        "#     messages = [{\"role\": \"system\", \"content\": system_message}] + history\n",
        "#     response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
        "#     image = None\n",
        "\n",
        "#     if response.choices[0].finish_reason==\"tool_calls\":                                         #use of tools\n",
        "#         message = response.choices[0].message\n",
        "#         response, city = handle_tool_call(message)\n",
        "#         messages.append(message)\n",
        "#         messages.append(response)\n",
        "#         image = artist(city)                                                                    #image generation - if model runs tool to find price of ticket, then image of city will be generated\n",
        "#         response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
        "\n",
        "#     reply = response.choices[0].message.content\n",
        "#     history += [{\"role\":\"assistant\", \"content\":reply}]\n",
        "\n",
        "#     # Comment out or delete the next line if you'd rather skip Audio for now..\n",
        "#     talker(reply)                                                                              #audio generation - speak response\n",
        "\n",
        "#     return history, image\n",
        "\n",
        "\n",
        "def chat(history):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}] + history\n",
        "    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
        "    image = None\n",
        "\n",
        "    if response.choices[0].finish_reason == \"tool_calls\":\n",
        "        message = response.choices[0].message\n",
        "        response, city = handle_tool_call(message)\n",
        "        messages.append(message)\n",
        "        messages.append(response)\n",
        "        image = artist(city)\n",
        "        response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
        "\n",
        "    reply = response.choices[0].message.content\n",
        "    history += [{\"role\": \"assistant\", \"content\": reply}]\n",
        "\n",
        "    audio_path = talker(reply)  # generate and return audio\n",
        "\n",
        "    return history, image, audio_path\n"
      ],
      "metadata": {
        "id": "8ZFA-laY2V59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More involved Gradio code as we're not using the preset Chat interface!\n",
        "# Passing in inbrowser=True in the last line will cause a Gradio window to pop up immediately.\n",
        "\n",
        "# with gr.Blocks() as ui:\n",
        "#     with gr.Row():\n",
        "#         chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
        "#         image_output = gr.Image(height=500)\n",
        "#     with gr.Row():\n",
        "#         entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
        "#     with gr.Row():\n",
        "#         clear = gr.Button(\"Clear\")\n",
        "\n",
        "#     def do_entry(message, history):\n",
        "#         history += [{\"role\":\"user\", \"content\":message}]\n",
        "#         return \"\", history\n",
        "\n",
        "#     entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(\n",
        "#         chat, inputs=chatbot, outputs=[chatbot, image_output]\n",
        "#     )\n",
        "#     clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
        "\n",
        "# ui.launch(inbrowser=True)\n",
        "\n",
        "\n",
        "with gr.Blocks() as ui:\n",
        "    with gr.Row():\n",
        "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
        "        image_output = gr.Image(height=500)\n",
        "        audio_output = gr.Audio(label=\"AI Voice Response\", autoplay=True)  # NEW!\n",
        "    with gr.Row():\n",
        "        entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
        "    with gr.Row():\n",
        "        clear = gr.Button(\"Clear\")\n",
        "\n",
        "    def do_entry(message, history):\n",
        "        history += [{\"role\": \"user\", \"content\": message}]\n",
        "        return \"\", history\n",
        "\n",
        "    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(\n",
        "        chat, inputs=chatbot, outputs=[chatbot, image_output, audio_output]\n",
        "    )\n",
        "    clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
        "\n",
        "ui.launch(inbrowser=True)"
      ],
      "metadata": {
        "id": "uaQO-_cwUomN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}